---
title: "ESM 244 - HW 3 - Task 1"
author: "Lauren Puffer"
format: html
editor: visual
embed-resources: true
code-fold: show
execute: 
  warning: false
  message: false
toc: TRUE
theme: journal
---

## Overview

What we gon' do???

## Load packages

```{r}
#packages for data organiztaion
library(tidyverse)
library(janitor)
library(here)

#packages for clustering
library(NbClust)
library(cluster)
library(factoextra)
library(dendextend)
library(ggdendro)
```

## Citation

Santa Barbara Coastal LTER and J. Melack. 2019. SBC LTER: Land: Stream chemistry in the Santa Barbara Coastal drainage area, ongoing since 2000 ver 16. Environmental Data Initiative. <https://doi.org/10.6073/pasta/67a558a24ceed9a0a5bf5e46ab841174>.

## Pseudocode

1.  Load data and change -999 to NA. Ensure that variables are numeric.
2.  Drop NAs using column deletion, then list deletion
3.  Include only numeric data.
4.  Scale data.
5.  Distance of scaled data.
6.  Use hclust to sort into clusters.
7.  Get correlation between clusters using complete linkages
8.  Create dendrogram to observe similarity of sites.

## Load stream chemistry data

The people who made this data set used -999 to convey a value where data is missing. We will change those values to NAs in our data wrangling. We will then take a look at our data to see how many NAs we have. Our agglomerative hierarchical cluster will also only work with numeric data. To ensure that all of the variables we use are being passed as numeric values we will convert them to numeric data.

```{r}

#load data from local file in R project
stream_data <- read.csv(here("data", "sbc_lter_registered_stream_chemistry.csv"))

#data wrangling: group by site and collect mean for clustering

#when we examine data in our console with the 'summary()' function, we see that there are -999 values for the minimum of many of our variables. This is how NAs are defined. We need to change the -999 values to NAs using the 'mutate()' function. 

stream_data <- read.csv(here("data","sbc_lter_registered_stream_chemistry.csv"),
                     na = '-999') |>
  clean_names()|>
  select(-timestamp_local)

#make sure columns with numeric data are being interpreted as numeric

#name columns to be used as numeric
cols_to_convert <- c("nh4_u_m", "no3_u_m", "po4_u_m", "tdn_u_m", 
                     "tdp_u_m", "tpc_u_m", "tpn_u_m", "tss_mgper_liter", 
                     "spec_cond_u_spercm")

#take these columns and convert them to numeric data 
stream_data[cols_to_convert] <- lapply(stream_data[cols_to_convert], as.numeric)

#rtest one column to see if it's numeric
is.numeric(stream_data$nh4_u_m)
#it worked!


#Now when we use 'summary()' we should not have -999 as our minimum. 
summary(stream_data)
```

## Deleting NAs

Because we have so many NAs in our dataset, we will need to get rid of as many NAs as possible, without deleting most of our usable data. To do this, we will first create a cut off for the amount of NAs a column should have. In this case, 9000 is selected because that is roughly 50% of our observations. Any column with greater than 9000 NAs will be dropped from the data set. Then we will perform a list deletion using drop_na() to get rid of the remainder of our NAs. The reason we are doing it in this order, is because if we only conducted a list deletion, we would have much fewer observations. We want to use as many observations as possible in our analysis for the sake of accuracy.

```{r}
#Because there are many NAs in our dataframe but we don't want to exclude too much data, we will do deletion of columns with >50 NAs. na_count <- colSums(is.na(df))

na_count <- colSums(is.na(stream_data))

#columns with more than 50% NAs
cols_to_remove <- names(na_count[na_count > 9000])

#remove columns with >50% NAs from the data frame
stream_col_clean <- stream_data[, !(names(stream_data) %in% cols_to_remove)]

#Summarize the cleaned data frame
summary(stream_col_clean)

#drop NAs 
stream_clean <- stream_col_clean |>
  drop_na()
```

While we have only 5% of the original data, our minimum values are 0 for the list delete method compared to the column delete method. We also were able to keep all of our variables. For this reason we will use the data frame with the list deletion method applied to get rid of our NAs.

## Summary data frame for each site 

The data frame we are going to make contains the mean values of each variable for each site.

```{r}
stream_summary <-  stream_clean|>
  group_by(site_code) |>
  summarize(across(where(is.numeric), ~ mean(.x, na.rm = TRUE)))
```

## Scaling data

Scaling ensures that all of our features are equally contributing to our analysis. This prevents variables with greater ranges from skewing our clusters in one way or another. In order to scale our data we must extract only the numeric values by removing non-numeric data. After we are done scaling, we will replace our site names.

```{r}
#Scale the numeric variables (columns 3:7)
stream_scaled <- stream_summary |>
  select(where(is.numeric))|>
  scale()

#put row names for site back into the dataframe
rownames(stream_scaled) <- stream_summary$site_code
```

## Calculating Euclidian distance

```{r}

#used vector of scaled data to calculate euclidian distance
euc_dist <- dist(stream_scaled, method= 'euclidian')
```

## Complete linkage hierarchical clustering

```{r}
#Hierarchical clustering (complete linkage)
comp_hclust_stream <- hclust(euc_dist, method = "complete" )
```

## Create dendrogram

```{r}
#Plot dendrogram
hclust_dend_stream<-ggdendrogram(comp_hclust_stream, 
             rotate = TRUE) + #rotates 90
  theme_minimal() +
  labs(title= "Complete", x = "Site")

hclust_dend_stream
```
