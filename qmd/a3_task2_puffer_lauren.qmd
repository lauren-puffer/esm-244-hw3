---
title: "ESM 244 - HW3 - Task 2"
author: "Lauren Puffer"
format: html
editor: visual
embed-resources: true
code-fold: show
execute: 
  warning: false
  message: false
toc: TRUE
theme: journal
---

## Overview

For this analysis, we will be using random forests in R to test how well a linear model using weather attributes like relative humidity, temperature, rain, and wind can predict the size of a burn area from a forest fire. Forest fire data used in this analysis comes from a meteorological station in Northeast Portugal, a place that has been heavily impacted by fire. We will compare our results to the data mining study conducted by Cortez and Morais in 2007. This study was conducted when Random Forests was still relatively new. Now, 18 years later, we are going to see if Random Forests has become better at improving model performance.

## Load packages

```{r}
library(tidymodels)
library(tidyverse)
library(ggcorrplot)
library(knitr)
library(kableExtra) #for creating a table with resulst from random forests
library(vip) #used for plotting variable importance
library(ranger)

#file management packages
library(janitor)
library(here)
```

## Citation

Cortez, P., & Morais, A. (2007, January). *A Data Mining Approach to Predict Forest Fires using Meteorological Data*. ResearchGate. <https://www.researchgate.net/publication/238767143_A_Data_Mining_Approach_to_Predict_Forest_Fires_using_Meteorological_Data>

## Explanation of variables

```{r}
var_table <- tibble(
  Variable = c("rh", "temp", "wind", "rain", "area"),
  Explanation = c("Outside relative humidity (%)", "Outside temperature (degrees C)",
                  "Outside wind speed (km/hr)", "Outside rain (mm/m^2)", "Total area burned (hectares)")
)

var_table |>
    kable(col.names = c("Variable", "Description"),
    format = "html",
    caption = "") |>
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed"))
```

## Pseudocode

1.  Split data maintaining balance

2.  Build recipe for regression

3.  Set engine

4.  Hyperparameter tuning on training set

<!-- -->

a.  Build and tune grid to go over parameters

b.  Use cross-validation to select best parameters to used in model

c.  Train model on best parameters selected

<!-- -->

5.  Evaluate model on test set

6.  Un-transform data

7.  Compare Mean Absolute Error from our regression in RF to the Cortez and Morais paper.

8.  Graph variable importance

## Load forest fire data

Because there are many zero values for area, we will perform a log10(+1) transformation to use Random Forests. After the parameters are determined for our linear regression, we will undo this transformation to collect our final accuracy metrics.

```{r}
fire_data <- read.csv(here("data","forestfires.csv")) |>
  clean_names() |>
  mutate(area = log10(area+1)) |>
  select(temp, rain, area, wind, rh) |>
  drop_na()
```

## Exploratory Data Analysis

Correlation of variables can be plotted using this correlation plot. This will allow us to see which parameters may be positively or negatively correlated with one another in a pairwise fashion.

```{r}
#| fig-cap: "Fig. 1 - Correlation plot of variables used in analysis. Variables can be positively or negatively correlated."

corr_plot <- fire_data |> 
  cor() |>
  ggcorrplot(
    method = "circle",
    type='upper',
    outline.col = "black", ) + 
  labs(x = "Variable 1", y = "Variable 2", title = "Correlation Plot")+
  scale_fill_gradient2(low = "deeppink", mid = "aquamarine3", high= "darkolivegreen2") +
  theme_bw() 

print(corr_plot)
```

## Split data

To train our model, we must set aside 75% of our data to be trained. The remaining 25% will be used to test the fit of our resulting model after using Random Forests.

```{r}
set.seed(666)

fire_split <- initial_split(fire_data, prop = 0.75, strata = area)

fire_train <- training(fire_split)

fire_test <- testing(fire_split)
```

## Creata a recipe with all predictors

When using a recipe in random forests, we want the machine to be able to pull any variables that are best suited for predicting area. We must specify that 'area' is our independent variable and that we are using our training data within this recipe. We will also set a correlation threshold of 90% to avoid over-fitting our data in our final model.

```{r}
fire_recipe <- recipe(area ~ ., data = fire_train) |> 
  step_zv(all_predictors()) |> 
  step_corr(all_predictors(), threshold = 0.9) #variables that are 90% or more w correlation
```

## Create a workflow and set up Random Forests

The workflow allows us to use our recipe with Random Forests. Random Forests will be used to optimize a linear regression model that predicts burn area. Our workflow will be used to create the best regression model. We will also tune our grid, which refers to the combination of parameters Random Forests will use to optimize our model performance.

```{r}
fire_spec <- rand_forest(trees = 1000, #number of decision trees used to make predictions. Accuracy and stibility of the model increases with the number of trees
                       mtry = tune(), #tuned on certain 'mtry' number of parameters
                       min_n=tune()) |> #minimum nodes or "split points"
  set_engine("ranger") |>
  set_mode("regression") #used for linear regression

fire_workflow <- workflow() |>
  add_recipe(fire_recipe) |> #recipe using all possible predictors to predict area
  add_model(fire_spec) #random forests set up
```

## Tune hyperparameters

To find the best combination of random variables selected and minimum nodes, Random Forests needs a specified range of hyperparameters to use to determine the best model. We have 4 variables in the weather attributes of our data, so our mtry cannot exceed 4. We want to avoid overfitting our data, so we will use 4 different values of minimum nodes. We will have a total of 12 ur combinations of random variables selected and minimum nodes used in our decision trees.

After expanding our grid, we will use our workflow, our grid, and a 5-fold cross-validation to train our model. We will collect Mean Absolute Error (MAE) to determine the accuracy of the models predictions with each combination of random variables and minimum nodes. This follows the protocol that Cortez and Morais used in Random Forests for their model selection.

```{r}
#create 12 possible gridsusing expand_grid() function
fire_grid <- expand_grid(
  mtry = seq(2, 4, by = 1), #use 3 possible vectors in mtry instead of 4 to avoid overfitting dataset
  min_n = seq(1, 10, by = 3)  
)

#tune grid to the workflow and resample
fire_res <- tune_grid(
  fire_workflow,
  resamples = vfold_cv(fire_train, v = 5),
  grid = fire_grid,
  metrics = metric_set(mae),
  control=control_grid(save_workflow = TRUE)  
)
```

## Model performance

Now we can see how well the model performed with different parameters: number of minimum nodes and variables used. The optimal selection of hyperparameters will have the lowest Mean Absolute Error.

```{r}
#| fig-cap: "Fig. 2 - Line graph showing the Mean Absolute Error calculate for different parameter combinations used in our grid after tuning."

fire_res |>
  collect_metrics() |>
  filter(.metric == "mae") |>
  mutate(min_n = factor(min_n)) |>
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, linewidth = 1.5) +
  geom_point() +
  labs(title= "Model Accuracy Determined by Hyperparameters",y = "Mean Absolute Error", x = "Number of Parameters Used") +
  scale_color_manual(values = c( "deeppink2", "darkorange", "goldenrod1", "darkolivegreen")) +
  theme_classic()
```

We can see that the best-performing model has the smallest number of variables (mtry), with the greatest number of minimum nodes (min_n). This combination uses 2 parameters with 10 minimum nodes. We will use those hyperparameters from our tuned grid to finalize our model and collect predictions of with Mean Absolute Error as our metric.

## Select and finalize the best fit model

We will pull our results from our plot and select the best fit combination of hyperparameters directly from it using the select_best() function.

Mtry (number of variables) for our model will be 2 and min_n (minimum nodes/decisions) will be 10.

We will now create our final model and use it in a workflow with our established recipe with our testing data to see how well our model predicts area using the parameters specified by our Random Forests workflow.

```{r}
#determine best number of min_n and mtry
fire_best<-select_best(fire_res,metric='mae')

fire_final<-finalize_model(fire_spec, fire_best)

#finalize workflow on testing dataset
final_fire_wf <- workflow() |>
  add_recipe(fire_recipe) |>
  add_model(fire_final)

final_fire_res <- final_fire_wf |>
  last_fit(fire_split)
```

## Get predictions and unlog the data

To ensure that our model worked properly on our testing data, we will look at the predictions.

These predictions were performed on transformed data, so afterwards we must un-transform our data in order to compare the MAE metrics we find with those that are in the Cortez and Morais paper from 2007.

```{r}
#collect predictions
prediction <- final_fire_res |>
  collect_predictions() 

#use mutate to unlog predictions and area
prediction_unlog <- prediction |>
  mutate(.pred = 10^.pred -1) |>
  mutate(area= 10^area -1)
```

## Calculate the MAE value

This MAE value will be the one we use to compare with the study's value.

```{r}
#Collect MAE predictions without log
mae_predictions <- prediction_unlog |>
  mae(truth = area, estimate = .pred)

#create table w/ mae predictions
mae_table <- tibble(
  Metric = c("MAE"),
  Model_Value = c(round(mae_predictions$.estimate, 2)),
  Comparison_Value = c("12.93Â±0.01")
)

mae_table |>
  kable(
    col.names = c('Metric', 'Model Value', 'Cortez and Morais, 2007'),
    caption = "Table 2 - Mean absolute error from our random forest linear regression and Cortez and Morais paper from 2007 using environmental attributes. Lower values of MAE suggest a better model."
  ) |>
  kable_styling()

```


Our model had a slightly higher Mean Absolute Error compared with the Cortez and Morais study. While this doesn't necessarily mean that our models are that different, it does suggest that Cortez and Morais were able to create a better model with random forests than we were.

## Graph of variable importance

To see which variables truly governed the predictions of our model, we will observe their importance visually.

```{r}
#| fig-cap: "Fig. 2 - Importance of weather attributes in Random Forests model."


importance_plot<- fire_final |>
  set_engine('ranger', importance = 'permutation') |> 
  fit(area ~ ., data = juice(prep(fire_recipe))) |> 
  vip(geom = 'point')

importance_point_plot_ <- importance_plot +
  labs(y = "Importance", x = "Variable") +
  theme_minimal() +  
  geom_point(color = "coral2", size = 3)

print(importance_point_plot_)
```

Temperature has the most importance of all the weather attributes in predicting burn area. This is consistent with the Cortez and Morais study, however, their second-most important variable was rain, whereas our second-most important variable is relative humidity.

## Conclusion

While our results differ slightly from the Cortez and Morais study, it is important to note that their study found that weather attributes were excellent predictors for smaller fires. This means that observations with smaller burn areas were more likely to be accuratle predicted by relative humidity, temperature, rain, and wind. Further analysis could examine why it is that weather attributes are not as great at predicting burn areas of larger fires. In the context of the data they used, Northeast Portugal sees a majority of small fires. With this in mind, a model like this could be very useful for distributing resource to manage smaller fires.
