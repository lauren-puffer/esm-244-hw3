---
title: "ESM 244 - HW3 - Task 2"
author: "Lauren Puffer"
format: html
editor: visual
embed-resources: true
code-fold: show
execute: 
  warning: false
  message: false
toc: TRUE
theme: journal
---

## Overview

For this analysis we will be using random forests in R to test how well a linear model using weather variables like relative humidity (%), temperature (celsius), rain, and wind can predict the size of a burn area (km\^2) from a forest fire. Forest fire data used in this analysis comes from a meteorological station in Northeast Portugal, a place which has been heavily impacted by fire. We will be comparing our results to the data mining study conduected by Cortez and Morais in 2007. This study was conducted when Random Forests was still relatively new. now, 18 years later, we are going to see if Random Forests has become better at improving model performance.

## Load packages

```{r}
library(tidymodels)
library(tidyverse)
library(ggcorrplot)
library(knitr)
library(kableExtra)
library(vip)
library(ranger)

#file management packages
library(janitor)
library(here)
```

## Citation

Cortez, P., & Morais, A. (2007, January). *A Data Mining Approach to Predict Forest Fires using Meteorological Data*. ResearchGate. <https://www.researchgate.net/publication/238767143_A_Data_Mining_Approach_to_Predict_Forest_Fires_using_Meteorological_Data>

## **Pseudocode**

1.  Split data maintaining balance

2.  Build recipe for regression

3.  Set engine

4.  Hyperparameter tuning on training set

<!-- -->

a.  Build and tune grid to go over parameters

b.  Use cross-validation to select best parameters to used in model

c.  Train model on best parameters selected

<!-- -->

5.  Evaluate model on test set

6.  Test for variable importance

##Load forest fire data Log 10(x+1)

```{r}
fire_data <- read.csv(here("data","forestfires.csv")) |>
  clean_names() |>
  mutate(area = log10(area+1)) |>
  select(temp, rain, area, wind, rh) |>
  drop_na()
```

##Exploratory Data Analysis

```{r}

corr_plot <- fire_data |> 
  cor() |>
  ggcorrplot(
    method = "circle",
    type='upper',
    outline.col = "black") + scale_fill_gradient2(low = "deeppink", mid = "aquamarine3", high= "darkolivegreen2") +
  theme_bw()

print(corr_plot)
```

## Split data

```{r}
set.seed(666)

fire_split <- initial_split(fire_data, prop = 0.75, strata = area)

fire_train <- training(fire_split)

fire_test <- testing(fire_split)
```

## Create recipe with all predictors

```{r}
fire_recipe <- recipe(area ~ ., data = fire_train) |> 
  step_zv(all_predictors()) |> 
  step_corr(all_predictors(), threshold = 0.9) #variables that are 90% or more w correlation
```

## Create a workflow and set up Random Forests

The workflow allows us to use our recipe with Random Forests. Random Forestst will be used to optimize a linear regression model that predicts burn area. Our workflow will be used to create the best regression model. We will also tune our grid using which refers to the combinatinon of parameters Random Forests will use to optimize our model performance.

```{r}
fire_spec <- rand_forest(trees = 1000, #number of decision trees used to make predictions. Accuracy and stibility of the model increases with the number of trees
                       mtry = tune(), #tuned on certain 'mtry' number of parameters
                       min_n=tune()) |> #minimum nodes or "split points"
  set_engine("ranger") |>
  set_mode("regression") #used for linear regression

fire_workflow <- workflow() |>
  add_recipe(fire_recipe) |> #recipe using all possible predictors to predict area
  add_model(fire_spec) #random forests set up
```

## Tune hyperparameters

To find the best combination of random variables selected and minimum nodes, Random Forests needs a specified range of hyperparameters to use to determine the best model. We have 4 variables in the weather attributes of our data, so our mtry cannot exceed 4. We want to avoid overfitting our data, so we will use 4 different values of minimum nodes. We will have a total of 12 ur combinations of random variables selected and minimum nodes used in our decision trees.

After expanding our grid, we will use our workflow, our grid, and a 5-fold cross-validation to train our model. We will collect Mean Absolute Error (MAE) to determine the accuracy of the models predictions with each combination of random variables and minimum nodes. This follows the protocol that Cortez and Morais used in Random Forests for their model selection.

```{r}
#create 12 possible gridsusing expand_grid() function
fire_grid <- expand_grid(
  mtry = seq(2, 4, by = 1), #use 3 possible vectors in mtry instead of 4 to avoid overfitting dataset
  min_n = seq(1, 10, by = 3)  
)

#tune grid to the workflow and resample
fire_res <- tune_grid(
  fire_workflow,
  resamples = vfold_cv(fire_train, v = 5),
  grid = fire_grid,
  metrics = metric_set(mae),
  control=control_grid(save_workflow = TRUE)  
)
```

## Model performance

Look at how well the model performed with different parameters

```{r}

fire_res |>
  collect_metrics() |>
  filter(.metric == "mae") |>
  mutate(min_n = factor(min_n)) |>
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, linewidth = 1.5) +
  geom_point() +
  labs(title= "",y = "Mean Absolute Error", x = "Number of Parameters Used") +
  scale_color_manual(values = c( "deeppink2", "darkorange", "goldenrod1", "darkolivegreen")) +
  theme_classic()
```

## Select the best fit

```{r}
#determine best number of min_n and mtry
fire_best<-select_best(fire_res,metric='mae')
```

Best number of Mtry is 1 and best number of min_n in 3.

## Finalize the model

```{r}

fire_final<-finalize_model(fire_spec, fire_best)

#finalize workflow on testing dataset
final_fire_wf <- workflow() |>
  add_recipe(fire_recipe) |>
  add_model(fire_final)

final_fire_res <- final_fire_wf |>
  last_fit(fire_split)
```

## Get predictions

```{r}
prediction <- final_fire_res |>
  collect_predictions() 

#table of predictions
```

## Unlog the data

```{r}
#use mutate to unlog predictions and area
prediction_unlog <- prediction |>
  mutate(.pred = 10^.pred) |>
  mutate(area= 10^area)
```

## Calculate the MAE value

```{r}
#Collect MAE predictions without log
mae_predictions <- prediction_unlog |>
  mae(truth = area, estimate = .pred)

```

## Create table w MAE value predictions

```{r}

```
